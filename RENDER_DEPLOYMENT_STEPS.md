# üöÄ Render Deployment Steps

## Pre-Deployment Checklist

### 1. Commit and Push Changes
```powershell
git add .
git commit -m "Fix: Memory optimization for Render deployment - Use Pinecone Inference API"
git push origin main
```

### 2. Verify Environment Variables in Render Dashboard

Go to your Render service ‚Üí **Environment** tab and ensure these are set:

#### Required API Keys (‚ö†Ô∏è MUST SET MANUALLY)
```
GROQ_API_KEY=gsk_xxxxxxxxxxxxx
PINECONE_API_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

#### Pinecone Settings
```
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=chatbot-rag-index
USE_PINECONE_INFERENCE=true
```

#### Database (auto-linked by Render)
```
DATABASE_URL=[Auto-generated by Render]
```

#### Security
```
SECRET_KEY=[Auto-generated by Render]
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

#### Application
```
ENVIRONMENT=production
TOP_K_RESULTS=5
FRONTEND_URL=https://your-frontend.vercel.app
```

### 3. Create Pinecone Index (if not exists)

1. Go to https://app.pinecone.io
2. Create new index with:
   - **Name:** `chatbot-rag-index`
   - **Dimensions:** `384` (for multilingual-e5-large)
   - **Metric:** `cosine`
   - **Region:** `us-east-1`

## Deployment Process

### Method 1: Auto-Deploy (Recommended)

1. Push changes to GitHub
2. Render will automatically detect and deploy
3. Monitor deployment logs in Render Dashboard

### Method 2: Manual Deploy

1. Go to Render Dashboard
2. Select your service
3. Click **Manual Deploy** ‚Üí **Deploy latest commit**

## Post-Deployment Verification

### 1. Check Deployment Logs

Look for these success messages:
```
‚úÖ Build successful
‚úÖ EmbeddingServicePinecone inicializado con modelo: multilingual-e5-large
‚úÖ Servicio Groq inicializado correctamente
‚úÖ Running 'uvicorn main:app --host 0.0.0.0 --port $PORT'
‚úÖ Application startup complete
```

### 2. Test Health Endpoints

```bash
# Health check
curl https://your-backend.onrender.com/health

# AI health
curl https://your-backend.onrender.com/ai_health/

# Chatbot info
curl https://your-backend.onrender.com/chatbot_info/
```

Expected responses:
```json
{"status":"healthy","timestamp":"..."}
{"status":"healthy","ai_provider":"Groq","model":"llama-3.1-8b-instant",...}
{"mode":"rag_enabled","rag_enabled":true,...}
```

### 3. Check Memory Usage

In Render Dashboard ‚Üí **Metrics**:
- Memory should be **~150-200MB** (not 512MB+)
- Should stay stable, not climbing

### 4. Test Chat Functionality

1. Log into frontend
2. Create a chatbot
3. Upload a document
4. Send a test message
5. Verify response uses document context

## Troubleshooting

### Error: "Out of memory"
**Cause:** Still loading heavy dependencies  
**Fix:** Check that `requirements-render.txt` doesn't have `sentence-transformers`, `torch`, or `transformers`

### Error: "PINECONE_API_KEY not found"
**Cause:** Environment variable not set  
**Fix:** Add in Render Dashboard ‚Üí Environment

### Error: "GROQ_API_KEY no v√°lida"
**Cause:** Invalid or missing Groq API key  
**Fix:** Get new key from https://console.groq.com/keys

### Error: "No open ports detected"
**Cause:** App not binding to PORT  
**Fix:** Verify uvicorn command uses `--port $PORT`

### Error: Database connection failed
**Cause:** DATABASE_URL not linked  
**Fix:** Check database is created and linked in render.yaml

## Performance Tips

### Optimize Cold Starts
Free tier apps sleep after 15 minutes of inactivity:
- First request after sleep takes ~30-60 seconds
- Consider upgrading to Starter plan ($7/month) for always-on

### Monitor Logs
```bash
# View live logs
# In Render Dashboard ‚Üí Logs tab
# Or use Render CLI:
render logs --tail=100
```

### Database Optimization
Free PostgreSQL tier limits:
- 256MB storage
- 20 connections
- Consider archiving old conversations periodically

## Rollback Plan

If deployment fails:

1. **Revert to previous deploy:**
   - Render Dashboard ‚Üí Select previous successful deployment
   - Click **Redeploy**

2. **Rollback code changes:**
   ```powershell
   git revert HEAD
   git push origin main
   ```

## Success Criteria

‚úÖ Deployment completes without errors  
‚úÖ Memory usage < 200MB  
‚úÖ Health endpoints return 200 OK  
‚úÖ Can create chatbot  
‚úÖ Can upload documents  
‚úÖ Can send messages and receive AI responses  
‚úÖ RAG context is used in responses  

## Next Steps After Successful Deployment

1. Update frontend environment variables:
   ```
   VITE_API_URL=https://your-backend.onrender.com
   ```

2. Deploy frontend to Vercel

3. Update CORS settings if needed:
   ```python
   # In main.py
   origins = [
       "https://your-frontend.vercel.app",
       # ...
   ]
   ```

4. Test end-to-end functionality

5. Set up monitoring/alerts (optional)

---

**Good luck with your deployment! üöÄ**

If you encounter any issues, check:
1. Render deployment logs
2. This guide's troubleshooting section
3. MEMORY_OPTIMIZATION.md for technical details
