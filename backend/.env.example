# Variables de entorno para el Chatbot RAG con IA Local
# Copia este archivo como .env y configura tus valores

# === CONFIGURACIÓN DE IA LOCAL ===
# Ollama - LLM Local (100% Gratuito, Sin API Keys)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b

# === CONFIGURACIÓN DE VECTOR DATABASE ===
# Pinecone - Almacenamiento de embeddings (Plan gratuito disponible)
# Obtén tu API key en: https://www.pinecone.io/
PINECONE_API_KEY=tu_api_key_de_pinecone_aqui

# === CONFIGURACIÓN DE BASE DE DATOS ===
# PostgreSQL local - Cambia estos valores según tu configuración
DATABASE_URL=postgresql://postgres:tu_password@localhost:5432/chatbot_uss_db

# === CONFIGURACIÓN JWT ===
# Clave secreta para tokens JWT - Genera una clave segura
SECRET_KEY=genera_una_clave_secreta_muy_segura_aqui_con_al_menos_32_caracteres

# Tiempo de expiración de tokens en minutos
ACCESS_TOKEN_EXPIRE_MINUTES=120

# === CONFIGURACIÓN DEL SERVIDOR ===
# Puerto del servidor (opcional, por defecto 8000)
PORT=8000

# Entorno de ejecución
ENVIRONMENT=development

# === INSTRUCCIONES DE CONFIGURACIÓN ===
# 
# 1. OLLAMA (IA Local - GRATIS):
#    - Instalar: winget install ollama (Windows) o curl -fsSL https://ollama.ai/install.sh | sh (Mac/Linux)
#    - Descargar modelo: ollama pull llama3.2:1b
#    - Iniciar servidor: ollama serve
#    - No requiere cambios en las variables OLLAMA_*
#
# 2. PINECONE (Vector DB - Plan gratuito disponible):
#    - Registrarse en: https://www.pinecone.io/
#    - Crear proyecto y obtener API key
#    - Reemplazar PINECONE_API_KEY con tu key real
#
# 3. POSTGRESQL (Base de datos local):
#    - Instalar PostgreSQL localmente
#    - Crear una base de datos (ej: chatbot_uss_db)
#    - Actualizar DATABASE_URL con tus credenciales
#
# 4. SECRET_KEY (Seguridad JWT):
#    - Generar una clave aleatoria segura
#    - Puedes usar: python -c "import secrets; print(secrets.token_hex(32))"
#
# === VENTAJAS DE ESTA CONFIGURACIÓN ===
# 
# ✅ IA 100% Local: Ollama ejecuta LLaMA en tu máquina
# ✅ Privacidad Total: Ningún texto se envía a servidores externos
# ✅ Sin Límites: Usa cuanto quieras sin restricciones de API
# ✅ Gratuito: Solo Pinecone requiere cuenta (plan gratuito generoso)
# ✅ Offline: El LLM funciona sin conexión a internet
# ✅ Rápido: Una vez cargado, responde instantáneamente
#
# === ARQUITECTURA RAG ===
#
# 1. Documentos PDF → Embeddings (Hugging Face - gratuito)
# 2. Embeddings → Pinecone (búsqueda vectorial)
# 3. Contexto relevante → Ollama Local (generación de respuesta)
# 4. Respuesta + Fuentes → Usuario
#
# Todo el procesamiento de texto se mantiene privado en tu máquina.
