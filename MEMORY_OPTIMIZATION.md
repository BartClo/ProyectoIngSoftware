# üöÄ Memory Optimization for Render Deployment

## Problem Fixed ‚úÖ

Your deployment was failing with **"Out of memory (used over 512Mi)"** because:

1. **Sentence Transformers** was loading a ~400MB ML model into RAM on startup
2. The free tier on Render only has **512MB RAM**
3. The app + model + dependencies exceeded this limit

## Solution Implemented

### ‚úÖ 1. Switched to Pinecone Inference API

**Before:**
```python
# ‚ùå Loads 400MB+ model locally
from services.embedding_service import embedding_service
```

**After:**
```python
# ‚úÖ Uses Pinecone's serverless embeddings (0 RAM)
from services.embedding_service_pinecone import embedding_service
```

### ‚úÖ 2. Removed Heavy Dependencies

**requirements-render.txt** now excludes:
- ‚ùå `sentence-transformers` (~400MB)
- ‚ùå `torch` (~500MB)
- ‚ùå `transformers` (~300MB)
- ‚ùå `numpy` (if not needed elsewhere)

**Total savings: ~1.2GB+**

### ‚úÖ 3. Optimized Uvicorn Settings

```yaml
startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT --workers 1 --timeout-keep-alive 30
```

- `--workers 1`: Single worker (saves ~100MB per additional worker)
- `--timeout-keep-alive 30`: Faster connection cleanup

### ‚úÖ 4. Added Memory Environment Variables

```yaml
- key: PYTHONUNBUFFERED
  value: "1"           # Prevents buffering (saves memory)
- key: MALLOC_ARENA_MAX
  value: "2"           # Limits malloc arenas (saves ~50-100MB)
```

### ‚úÖ 5. Optimized Build Command

```yaml
buildCommand: pip install --no-cache-dir -r requirements-render.txt
```

`--no-cache-dir`: Prevents pip from caching packages during build

## Files Modified

1. ‚úÖ `backend/requirements-render.txt` - Removed heavy ML dependencies
2. ‚úÖ `backend/routes/documents.py` - Changed import to Pinecone service
3. ‚úÖ `backend/routes/chat_rag.py` - Changed import to Pinecone service
4. ‚úÖ `render.yaml` - Added memory optimizations

## How Pinecone Inference API Works

Instead of loading models locally:

```python
# ‚ùå OLD: Loads model in memory
model = SentenceTransformer("all-MiniLM-L6-v2")  # ~400MB RAM
embeddings = model.encode(texts)

# ‚úÖ NEW: Uses Pinecone's serverless API (0 local RAM)
embeddings = pc.inference.embed(
    model="multilingual-e5-large",
    inputs=texts,
    parameters={"input_type": "passage"}
)
```

**Benefits:**
- ‚úÖ **0 RAM usage** for embeddings
- ‚úÖ Faster startup time
- ‚úÖ Better model (multilingual-e5-large)
- ‚úÖ Free on Pinecone

## Expected Memory Usage

### Before Optimization
- FastAPI + dependencies: ~100MB
- Sentence Transformers model: ~400MB
- PyTorch: ~200MB
- **Total: ~700MB** ‚ùå (exceeds 512MB limit)

### After Optimization
- FastAPI + dependencies: ~100MB
- Pinecone client: ~10MB
- Groq client: ~5MB
- SQLAlchemy + DB: ~30MB
- **Total: ~145MB** ‚úÖ (well under 512MB limit)

**Memory headroom: ~367MB (71% free)**

## Next Steps

### 1. Verify in Render Dashboard

After deployment, check:
- **Settings ‚Üí Environment** - Verify all API keys are set
- **Logs** - Should show successful startup without "Out of memory"
- **Metrics** - Memory usage should be ~150-200MB

### 2. Required Environment Variables

Make sure these are set in Render Dashboard:

```bash
GROQ_API_KEY=gsk_xxxxx...
PINECONE_API_KEY=xxxxx...
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=chatbot-rag-index
USE_PINECONE_INFERENCE=true
```

### 3. Monitor Memory Usage

Use Render's metrics to monitor:
- Should stay under **200MB** during normal operation
- Spikes during document processing are normal
- If exceeds **400MB**, investigate memory leaks

## Troubleshooting

### If deployment still fails:

1. **Check logs for other errors:**
   ```bash
   render logs --tail=100
   ```

2. **Verify Pinecone API key is valid:**
   - Log into https://app.pinecone.io
   - Check API Keys section
   - Make sure index exists

3. **Check Groq API key:**
   - Log into https://console.groq.com/keys
   - Verify key is active

4. **Database connection:**
   - Ensure `DATABASE_URL` is properly linked in Render

## Performance Impact

**Embedding Generation Speed:**
- **Before:** ~100ms (local Sentence Transformers)
- **After:** ~150-200ms (Pinecone Inference API)

**Trade-off:** Slightly slower embeddings (~50-100ms) but **stable deployment** with **zero RAM usage** for ML models.

## Additional Optimizations (if needed)

If you need even more memory savings:

### Option 1: Remove bcrypt (saves ~20MB)
```python
# Use simple hashing instead of bcrypt
passlib[bcrypt] ‚Üí passlib
```

### Option 2: Lazy import heavy modules
```python
# Only import when needed
def process_document():
    from pypdf import PdfReader  # Import only when called
    ...
```

### Option 3: Upgrade to Starter Plan ($7/month)
- 512MB ‚Üí **2GB RAM**
- Would allow local models if needed
- More workers for better performance

## Verification Checklist

- [x] Removed Sentence Transformers from requirements-render.txt
- [x] Changed imports to use embedding_service_pinecone
- [x] Added memory optimization environment variables
- [x] Optimized uvicorn command
- [x] Set USE_PINECONE_INFERENCE=true
- [ ] Verify PINECONE_API_KEY is set in Render Dashboard
- [ ] Verify GROQ_API_KEY is set in Render Dashboard
- [ ] Deploy and check logs for successful startup
- [ ] Test chat functionality with documents

## Success Indicators

You'll know it's working when:

1. ‚úÖ Deployment completes without "Out of memory" error
2. ‚úÖ Logs show: "EmbeddingServicePinecone inicializado con modelo: multilingual-e5-large"
3. ‚úÖ Health check returns 200 OK
4. ‚úÖ Chat with documents works correctly
5. ‚úÖ Memory usage stays under 200MB in Render metrics

---

**Created:** 2025-11-12  
**Status:** Ready to deploy üöÄ
