# Chatbot RAG para Docentes - Universidad San Sebasti√°n

Este repositorio contiene el c√≥digo del proyecto desarrollado en **Taller de Ingenier√≠a de Software**.  
El objetivo es construir un chatbot especializado con sistema RAG (Retrieval-Augmented Generation) que responde preguntas de docentes bas√°ndose exclusivamente en documentos institucionales cargados.

## üéØ Caracter√≠sticas del Sistema RAG

- **üß† IA Local**: Usa Ollama con LLaMA 3.2 (100% gratuito, sin API keys, privacidad total)
- **üìù Embeddings eficientes**: Usa Hugging Face (gratuito) para generar embeddings una sola vez
- **üíæ Cach√© inteligente**: Evita regenerar embeddings, solo procesa cuando hay cambios
- **‚òÅÔ∏è Vector Database**: Almacena embeddings en Pinecone para b√∫squedas r√°pidas
- **üîí Privacidad**: El LLM se ejecuta completamente en tu m√°quina local
- **üìö Fuentes transparentes**: Muestra las fuentes de informaci√≥n en cada respuesta
- **‚ö° Sin l√≠mites**: No hay restricciones de cuota ni costos por uso

## üöÄ Stack Tecnol√≥gico

- **Frontend:** React + TypeScript + Vite
- **Backend:** FastAPI (Python)
- **Base de Datos:** PostgreSQL
- **Sistema RAG:** 
  - **LLM Local:** Ollama con LLaMA 3.2 1B (gratuito, privado)
  - **Embeddings:** Hugging Face Sentence Transformers (all-MiniLM-L6-v2)
  - **Vector DB:** Pinecone
- **Deployment:** Docker Compose

## üìÇ Estructura del Proyecto

```
‚îú‚îÄ‚îÄ frontend/                 # Aplicaci√≥n React con TypeScript
‚îú‚îÄ‚îÄ backend/                  # API FastAPI con sistema RAG
‚îÇ   ‚îú‚îÄ‚îÄ context_docs/        # Documentos PDF para el RAG
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_cache/    # Cach√© local de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ rag_manager.py       # Gestor principal del RAG
‚îÇ   ‚îú‚îÄ‚îÄ embedding_service.py # Servicio de embeddings con HF
‚îÇ   ‚îú‚îÄ‚îÄ pinecone_service.py  # Servicio de Pinecone
‚îÇ   ‚îú‚îÄ‚îÄ setup_rag.py         # Script de configuraci√≥n inicial
‚îÇ   ‚îî‚îÄ‚îÄ test_rag.py          # Pruebas del sistema RAG
‚îú‚îÄ‚îÄ db/                      # Scripts SQL y configuraci√≥n
‚îî‚îÄ‚îÄ docker-compose.yml       # Orquestaci√≥n de servicios
```

## ‚ö° Inicio R√°pido

### 1. Clonar el Repositorio
```bash
git clone https://github.com/BartClo/ProyectoIngSoftware.git
cd ProyectoIngSoftware
```

### 2. Instalar Ollama (LLM Local)

#### Windows:
```bash
winget install ollama
```

#### macOS/Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Descargar modelo LLaMA 3.2:
```bash
ollama pull llama3.2:1b
```

### 3. Configurar Backend RAG

#### Instalar dependencias:
```bash
cd backend
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux
pip install -r requirements.txt
```

#### Configurar variables de entorno:
```bash
cp .env.example .env
# Editar .env con tu configuraci√≥n
```

Variables necesarias en `.env`:
```env
# IA Local con Ollama (NO requiere API keys pagadas)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b

# Vector Database (requiere cuenta gratuita)
PINECONE_API_KEY=tu_api_key_de_pinecone

# Base de datos local
DATABASE_URL=postgresql://postgres:tu_password@localhost:5432/tu_database

# JWT
SECRET_KEY=tu_clave_secreta_jwt
```

#### Obtener API Key de Pinecone (GRATIS):

**Pinecone:**
1. Reg√≠strate en [Pinecone](https://www.pinecone.io/) (plan gratuito disponible)
2. Crea un proyecto y obt√©n tu API key
3. Agrega a `PINECONE_API_KEY`

> **üí° Nota:** Pinecone ofrece un plan gratuito generoso. Solo Ollama es completamente local.

#### Agregar documentos:
```bash
# Coloca tus PDFs en backend/context_docs/
cp tu_documento.pdf backend/context_docs/
```

#### Inicializar sistema RAG:
```bash
cd backend
python setup_rag.py
```

### 4. Ejecutar el Sistema

#### Opci√≥n A: Con Docker (Recomendado)
```bash
# Iniciar Ollama primero
ollama serve  # En una terminal separada

# Luego el resto del sistema
docker-compose up --build
```

#### Opci√≥n B: Desarrollo Local
```bash
# Terminal 1 - Ollama
ollama serve

# Terminal 2 - Backend
cd backend
venv\Scripts\activate  # Windows
uvicorn main:app --reload

# Terminal 3 - Frontend
cd frontend
npm install
npm run dev
```

### 5. Acceder a la Aplicaci√≥n

- **Frontend:** http://localhost:5173
- **Backend API:** http://localhost:8000
- **Documentaci√≥n API:** http://localhost:8000/docs
- **Ollama:** http://localhost:11434 (servidor local)

## üîß Uso del Sistema

### Primera Configuraci√≥n

1. **Instalar Ollama y modelo**: `ollama pull llama3.2:1b`
2. **Agregar documentos PDF** en `backend/context_docs/`
3. **Ejecutar setup**: `python backend/setup_rag.py`
4. **Verificar funcionamiento**: `python backend/test_rag.py`

### Funcionamiento del RAG

1. **Usuario hace pregunta** ‚Üí El frontend env√≠a al backend
2. **Sistema busca contexto** ‚Üí Genera embedding de la pregunta y busca en Pinecone
3. **Encuentra documentos relevantes** ‚Üí Filtra por score de similitud
4. **Ollama genera respuesta localmente** ‚Üí Usa solo el contexto encontrado + historial de chat
5. **Respuesta incluye fuentes** ‚Üí Muestra qu√© documentos se usaron
6. **100% privado** ‚Üí Todo el procesamiento del LLM es local

### Administraci√≥n

- **Ver estado del sistema**: `GET /rag_status/`
- **Reconstruir √≠ndice**: `POST /rebuild_index/`
- **Informaci√≥n del chatbot**: `GET /chatbot_info/`
- **Estado de Ollama**: `ollama list` para ver modelos instalados

## üéØ Caracter√≠sticas T√©cnicas

### IA Local con Ollama
- **LLaMA 3.2 1B**: Modelo optimizado para respuestas r√°pidas y precisas
- **Ejecuci√≥n local**: Sin env√≠o de datos a servidores externos
- **Sin l√≠mites**: Usa cuanto necesites sin restricciones de API
- **Privacidad garantizada**: Todos los datos permanecen en tu m√°quina

### Sistema de Cach√© Inteligente
- Detecta cambios en documentos PDF
- Solo regenera embeddings cuando es necesario
- Almacena metadatos para verificaci√≥n de integridad

### B√∫squeda Sem√°ntica Optimizada
- Modelo: `all-MiniLM-L6-v2` (384 dimensiones, gratuito)
- Similitud coseno con filtro por score m√≠nimo
- Top-k documentos m√°s relevantes

### Generaci√≥n Contextual Local
- Combina contexto de documentos + historial conversacional
- Respuestas basadas exclusivamente en la base de conocimientos
- Transparencia total con fuentes citadas
- **100% offline para generaci√≥n de texto**

## üß™ Pruebas y Validaci√≥n

```bash
# Ejecutar pruebas completas
cd backend
python test_rag.py

# Verificar estado del sistema
curl "http://localhost:8000/rag_status/"

# Ejemplo de consulta
curl -X POST "http://localhost:8000/conversations/1/messages/" \
  -H "Authorization: Bearer tu_token" \
  -H "Content-Type: application/json" \
  -d '{"text": "¬øCu√°les son las pol√≠ticas de calidad?"}'
```

## üìñ Documentaci√≥n Adicional

- **[Gu√≠a Completa de Configuraci√≥n RAG](backend/RAG_SETUP_GUIDE.md)** - Documentaci√≥n detallada del sistema
- **[API Documentation](http://localhost:8000/docs)** - Swagger/OpenAPI docs
- **[Deployment Guide](DEPLOYMENT_GUIDE.md)** - Gu√≠a de despliegue en producci√≥n

## ÔøΩ Desarrollo

### Estructura del C√≥digo RAG

- **`rag_manager.py`**: Orquestador principal del sistema RAG
- **`embedding_service.py`**: Gesti√≥n de embeddings con Hugging Face
- **`pinecone_service.py`**: Interfaz con Pinecone Vector Database
- **`main.py`**: API FastAPI con endpoints RAG integrados

### Personalizaci√≥n

```python
# Cambiar modelo de embeddings
embedding_service = EmbeddingService(model_name="tu-modelo-preferido")

# Ajustar par√°metros de b√∫squeda
similar_docs = search_context(query, top_k=10, min_score=0.6)

# Configurar Pinecone
pinecone_service = PineconeService(index_name="mi-indice-personalizado")
```

## üö® Soluci√≥n de Problemas

### Error: "Sistema RAG no inicializado"
```bash
cd backend
python setup_rag.py
```

### Error: "No se pudo conectar a Ollama"
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# En otra terminal, verificar modelos
ollama list

# Si no tienes el modelo, descargarlo
ollama pull llama3.2:1b
```

### Error: "API keys no encontradas"
- Verifica que el archivo `.env` existe y tiene las variables correctas
- Solo necesitas `PINECONE_API_KEY` (Ollama es local)
- Reinicia el servidor despu√©s de cambiar `.env`

### Documentos no procesan correctamente
- Aseg√∫rate de que los PDFs contienen texto (no solo im√°genes)
- Verifica que est√°n en `backend/context_docs/`

### Ollama funciona lento
- La primera respuesta puede ser lenta (carga del modelo)
- Considera usar `llama3.2:3b` si tienes m√°s RAM
- El modelo se mantiene en memoria despu√©s del primer uso

## üë• Equipo

- **Luciano Alegria** - Desarrollo Frontend
- **Renata Antequiera** - Desarrollo Backend  
- **Marcelo Mu√±oz** - Integraci√≥n RAG y DevOps

## üìÑ Licencia

Este proyecto fue desarrollado como parte del Taller de Ingenier√≠a de Software en la Universidad San Sebasti√°n.

---

üéâ **¬°Tu chatbot RAG est√° listo para responder preguntas bas√°ndose en tus documentos institucionales!**

